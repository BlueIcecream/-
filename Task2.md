过拟合、欠拟合及其解决方案
===============

* 训练误差：模型在训练数据集上表现出的误差

* 泛化误差：模型在任意一个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似

* 欠拟合：模型没有得到很低的训练误差

* 过拟合：模型的测试误差远高于训练误差

k折交叉验证
-----

由于验证数据集不参与模型训练，当训练数据不够用时，预留大量的验证数据显得太奢侈。一种改善的方法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把原始训练数据集分割成K个不重合的子数据集，然后我们做K次模型训练和验证。每一次，我们使用一个子数据集验证模型，并使用其他K-1个子数据集来训练模型。在这K次训练和验证中，每次用来验证模型的子数据集都不同。最后，我们对这K次训练误差和验证误差分别求平均。

训练数据集的大小
-----

* 如果训练样本过少，过拟合很容易发生。

* 在资源允许的范围内，通常希望训练数据集更大，尤其是模型的复杂度较高。

权重衰减
----

权重衰减等价于  L2  范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常用手段。

L2 范数正则化（regularization）
----

L2 范数正则化在模型原损失函数基础上添加L2范数惩罚项，从而得到训练所需要最小化的函数

丢弃法
----

多层感知机中神经网络图描述了一个单隐藏层的多层感知机，该层的隐藏单元将有一定概率被丢弃掉。

梯度消失、梯度爆炸
==========

当神经网络的层数较多时，模型的数值稳定性容易变差

如果权重参数较小多层之后累乘梯度容易消失

如果权重参数较大多层之后类乘梯度容易爆炸

协变量偏移
----

特征分布的变化，训练数据与测试数据的数据集分布不同

标签偏移
----

测试数据中存在测试数据中不存在的标签

概念偏移
----

在不同的数据集中相同内容的标签不同

循环神经网络进阶
=====

GRU
--

⻔控循环神经⽹络：捕捉时间序列中时间步距离较⼤的依赖关系


* 重置⻔有助于捕捉时间序列⾥短期的依赖关系；

* 更新⻔有助于捕捉时间序列⾥⻓期的依赖关系。

LSTM长短期记忆
-----

* 遗忘门:控制上一时间步的记忆细胞 输入门:控制当前时间步的输入

* 输出门:控制从记忆细胞到隐藏状态

* 记忆细胞：⼀种特殊的隐藏状态的信息的流动

深度循环神经网络
----

增加层数


双向循环神经网络
----

前向Ht与后向的Ht用concat进行连接


