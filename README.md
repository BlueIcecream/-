线性回归
========

模型
--------

输入与输出之间的线性关系

数据集
------

含有特征和标签的数据集合

训练数据/测试数据

损失函数
-------
非负误差，通常为平方函数
优化函数
------

* 解析解：用公式直接表达

* 数值解：优化算法有限次迭代模型参数尽可能降低损失函数的值

* 学习率：每次优化中，学习的步长大小

向量相加
-------

* 向量按元素逐一做标量加法
* 做矢量加法

[cdoe](code/向量计算.py)


线性回归模型从零开始的实现
--------------------------

能够更好的理解模型和神经网络底层的原理

[cdoe](code/线性回归模型从零实现.py)

线性回归模型使用pytorch的简洁实现
-------

能够更加快速地完成模型的设计与实现

[code](code/线性回归模型使用pytorch的简洁实现.py)


softmax和分类模型
=======
softmax基本概念
-----
* 分类

使用离散数值表示类别

* 神经网络图
softmax回归同线性回归一样，也是一个单层神经网络
softmax回归的输出层也是一个全连接层。

* 输出问题

1.输出层的范围不统一

2.离散值不缺丢范围的输出值之间的误差

使用softmax将输出值变换成正且和为一的分布概率

* 计算效率

1.将单样本进行矢量计算来表达

2.小批量数据做矢量运算

交叉熵损失函数
-----

* 平方损失估计

我们只需要其中一个比其他预测值和大就行了，不管其他两个预测值为多少，类别预测均正确。
而平方损失则过于严格，俩种=者分类正确的情况下损失会升高。

* 交叉熵

交叉熵只关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。当然，遇到一个样本有多个标签时，例如图像里含有不止一个物体时，我们并不能做这一步简化。但即便对于这种情况，交叉熵同样只关心对图像中出现的物体类别的预测概率。

获取Fashion-MNIST训练集和读取数据
---
[code](code/获取Fashion-MNIST训练集和读取数据.py)

softmax从零开始的实现
------
[code](code/softmax从零开始的实现.py)
    
softmax的简洁实现
-----
[code](code/softmax的简洁实现.py)
